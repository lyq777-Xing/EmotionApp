{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 80000/80000 [00:00<00:00, 319411.18 examples/s]\n",
      "Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 20000/20000 [00:00<00:00, 297211.21 examples/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 1 2 3 4]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-multilingual-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 80000/80000 [02:53<00:00, 459.78 examples/s]\n",
      "Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 20000/20000 [00:41<00:00, 476.64 examples/s]\n",
      "e:\\demo\\EmotionApp\\backend\\EmotionAppBackend\\Algorithm\\HuggingFace\\hf\\Lib\\site-packages\\transformers\\training_args.py:1611: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ğŸ¤— Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "CUDA error: device-side assert triggered\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mRuntimeError\u001b[39m                              Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[15]\u001b[39m\u001b[32m, line 70\u001b[39m\n\u001b[32m     55\u001b[39m training_args = TrainingArguments(\n\u001b[32m     56\u001b[39m     output_dir=\u001b[33m\"\u001b[39m\u001b[33m./results\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m     57\u001b[39m     evaluation_strategy=\u001b[33m\"\u001b[39m\u001b[33mepoch\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m     66\u001b[39m     save_total_limit=\u001b[32m2\u001b[39m,\n\u001b[32m     67\u001b[39m )\n\u001b[32m     69\u001b[39m \u001b[38;5;66;03m# ========== 6. å®šä¹‰ Trainer ==========\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m70\u001b[39m trainer = \u001b[43mTrainer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     71\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     72\u001b[39m \u001b[43m    \u001b[49m\u001b[43margs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtraining_args\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     73\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtrain_dataset\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtrain_dataset\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     74\u001b[39m \u001b[43m    \u001b[49m\u001b[43meval_dataset\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtest_dataset\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     75\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcompute_metrics\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcompute_metrics\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     76\u001b[39m \u001b[43m)\u001b[49m\n\u001b[32m     78\u001b[39m \u001b[38;5;66;03m# ========== 7. è®­ç»ƒæ¨¡å‹ ==========\u001b[39;00m\n\u001b[32m     79\u001b[39m train_results = trainer.train()\n",
      "\u001b[36mFile \u001b[39m\u001b[32me:\\demo\\EmotionApp\\backend\\EmotionAppBackend\\Algorithm\\HuggingFace\\hf\\Lib\\site-packages\\transformers\\utils\\deprecation.py:172\u001b[39m, in \u001b[36mdeprecate_kwarg.<locals>.wrapper.<locals>.wrapped_func\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    168\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m minimum_action \u001b[38;5;129;01min\u001b[39;00m (Action.NOTIFY, Action.NOTIFY_ALWAYS) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_torchdynamo_compiling():\n\u001b[32m    169\u001b[39m     \u001b[38;5;66;03m# DeprecationWarning is ignored by default, so we use FutureWarning instead\u001b[39;00m\n\u001b[32m    170\u001b[39m     warnings.warn(message, \u001b[38;5;167;01mFutureWarning\u001b[39;00m, stacklevel=\u001b[32m2\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m172\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32me:\\demo\\EmotionApp\\backend\\EmotionAppBackend\\Algorithm\\HuggingFace\\hf\\Lib\\site-packages\\transformers\\trainer.py:457\u001b[39m, in \u001b[36mTrainer.__init__\u001b[39m\u001b[34m(self, model, args, data_collator, train_dataset, eval_dataset, processing_class, model_init, compute_loss_func, compute_metrics, callbacks, optimizers, optimizer_cls_and_kwargs, preprocess_logits_for_metrics)\u001b[39m\n\u001b[32m    455\u001b[39m \u001b[38;5;28mself\u001b[39m.compute_loss_func = compute_loss_func\n\u001b[32m    456\u001b[39m \u001b[38;5;66;03m# Seed must be set before instantiating the model when using model\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m457\u001b[39m enable_full_determinism(\u001b[38;5;28mself\u001b[39m.args.seed) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.args.full_determinism \u001b[38;5;28;01melse\u001b[39;00m \u001b[43mset_seed\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43margs\u001b[49m\u001b[43m.\u001b[49m\u001b[43mseed\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    459\u001b[39m \u001b[38;5;28mself\u001b[39m.hp_name = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    460\u001b[39m \u001b[38;5;28mself\u001b[39m.deepspeed = \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32me:\\demo\\EmotionApp\\backend\\EmotionAppBackend\\Algorithm\\HuggingFace\\hf\\Lib\\site-packages\\transformers\\trainer_utils.py:105\u001b[39m, in \u001b[36mset_seed\u001b[39m\u001b[34m(seed, deterministic)\u001b[39m\n\u001b[32m    103\u001b[39m np.random.seed(seed)\n\u001b[32m    104\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m is_torch_available():\n\u001b[32m--> \u001b[39m\u001b[32m105\u001b[39m     \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmanual_seed\u001b[49m\u001b[43m(\u001b[49m\u001b[43mseed\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    106\u001b[39m     torch.cuda.manual_seed_all(seed)\n\u001b[32m    107\u001b[39m     \u001b[38;5;66;03m# ^^ safe to call this function even if cuda is not available\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32me:\\demo\\EmotionApp\\backend\\EmotionAppBackend\\Algorithm\\HuggingFace\\hf\\Lib\\site-packages\\torch\\_compile.py:32\u001b[39m, in \u001b[36m_disable_dynamo.<locals>.inner\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m     29\u001b[39m     disable_fn = torch._dynamo.disable(fn, recursive)\n\u001b[32m     30\u001b[39m     fn.__dynamo_disable = disable_fn\n\u001b[32m---> \u001b[39m\u001b[32m32\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mdisable_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32me:\\demo\\EmotionApp\\backend\\EmotionAppBackend\\Algorithm\\HuggingFace\\hf\\Lib\\site-packages\\torch\\_dynamo\\eval_frame.py:745\u001b[39m, in \u001b[36mDisableContext.__call__.<locals>._fn\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    741\u001b[39m prior_skip_guard_eval_unsafe = set_skip_guard_eval_unsafe(\n\u001b[32m    742\u001b[39m     _is_skip_guard_eval_unsafe_stance()\n\u001b[32m    743\u001b[39m )\n\u001b[32m    744\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m745\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    746\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    747\u001b[39m     _maybe_set_eval_frame(prior)\n",
      "\u001b[36mFile \u001b[39m\u001b[32me:\\demo\\EmotionApp\\backend\\EmotionAppBackend\\Algorithm\\HuggingFace\\hf\\Lib\\site-packages\\torch\\random.py:46\u001b[39m, in \u001b[36mmanual_seed\u001b[39m\u001b[34m(seed)\u001b[39m\n\u001b[32m     43\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mcuda\u001b[39;00m\n\u001b[32m     45\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m torch.cuda._is_in_bad_fork():\n\u001b[32m---> \u001b[39m\u001b[32m46\u001b[39m     \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcuda\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmanual_seed_all\u001b[49m\u001b[43m(\u001b[49m\u001b[43mseed\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     48\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mmps\u001b[39;00m\n\u001b[32m     50\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m torch.mps._is_in_bad_fork():\n",
      "\u001b[36mFile \u001b[39m\u001b[32me:\\demo\\EmotionApp\\backend\\EmotionAppBackend\\Algorithm\\HuggingFace\\hf\\Lib\\site-packages\\torch\\cuda\\random.py:127\u001b[39m, in \u001b[36mmanual_seed_all\u001b[39m\u001b[34m(seed)\u001b[39m\n\u001b[32m    124\u001b[39m         default_generator = torch.cuda.default_generators[i]\n\u001b[32m    125\u001b[39m         default_generator.manual_seed(seed)\n\u001b[32m--> \u001b[39m\u001b[32m127\u001b[39m \u001b[43m_lazy_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcb\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mseed_all\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32me:\\demo\\EmotionApp\\backend\\EmotionAppBackend\\Algorithm\\HuggingFace\\hf\\Lib\\site-packages\\torch\\cuda\\__init__.py:249\u001b[39m, in \u001b[36m_lazy_call\u001b[39m\u001b[34m(callable, **kwargs)\u001b[39m\n\u001b[32m    247\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_lazy_call\u001b[39m(\u001b[38;5;28mcallable\u001b[39m, **kwargs):\n\u001b[32m    248\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m is_initialized():\n\u001b[32m--> \u001b[39m\u001b[32m249\u001b[39m         \u001b[38;5;28;43mcallable\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    250\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    251\u001b[39m         \u001b[38;5;66;03m# TODO(torch_deploy): this accesses linecache, which attempts to read the\u001b[39;00m\n\u001b[32m    252\u001b[39m         \u001b[38;5;66;03m# file system to get traceback info. Patch linecache or do something\u001b[39;00m\n\u001b[32m    253\u001b[39m         \u001b[38;5;66;03m# else here if this ends up being important.\u001b[39;00m\n\u001b[32m    254\u001b[39m         \u001b[38;5;28;01mglobal\u001b[39;00m _lazy_seed_tracker\n",
      "\u001b[36mFile \u001b[39m\u001b[32me:\\demo\\EmotionApp\\backend\\EmotionAppBackend\\Algorithm\\HuggingFace\\hf\\Lib\\site-packages\\torch\\cuda\\random.py:125\u001b[39m, in \u001b[36mmanual_seed_all.<locals>.cb\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m    123\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(device_count()):\n\u001b[32m    124\u001b[39m     default_generator = torch.cuda.default_generators[i]\n\u001b[32m--> \u001b[39m\u001b[32m125\u001b[39m     \u001b[43mdefault_generator\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmanual_seed\u001b[49m\u001b[43m(\u001b[49m\u001b[43mseed\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[31mRuntimeError\u001b[39m: CUDA error: device-side assert triggered\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from datasets import load_dataset\n",
    "from transformers import BertTokenizer, BertForSequenceClassification, Trainer, TrainingArguments\n",
    "from sklearn.metrics import accuracy_score\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# ========== 1. åŠ è½½æ•°æ®é›† ==========\n",
    "# ä¸‹ä¸€ä¸ªä½¿ç”¨mteb/amazon_reviews_multiè®­ç»ƒ\n",
    "dataset = load_dataset(\"codyburker/yelp_review_sampled\")\n",
    "# ========== 2. æ•°æ®å­—æ®µé‡å‘½å ==========\n",
    "def rename_fields(examples):\n",
    "    examples[\"label\"] = examples[\"stars\"]  # å°† 'stars' å­—æ®µæ”¹åä¸º 'label'\n",
    "    del examples[\"stars\"]  # åˆ é™¤åŸæ¥çš„ 'stars' å­—æ®µ\n",
    "    return examples\n",
    "train_dataset = dataset[\"train\"].map(rename_fields)\n",
    "test_dataset = dataset[\"test\"].map(rename_fields)\n",
    "\n",
    "# å¤„ç†labelçš„èµ·å§‹æ•°å­—\n",
    "def adjust_labels(examples):\n",
    "    # å°†æ ‡ç­¾å‡å» 1ï¼Œä½¿æ ‡ç­¾èŒƒå›´å˜ä¸º [0, 1, 2, 3, 4]\n",
    "    examples[\"label\"] = [label - 1 for label in examples[\"label\"]]  # å¦‚æœæ˜¯åˆ—è¡¨æƒ…å†µ\n",
    "    return examples\n",
    "\n",
    "# é‡æ–°åº”ç”¨æ ‡ç­¾è°ƒæ•´å‡½æ•°ï¼Œç¡®ä¿æ˜¯é€ä¸ªæ ·æœ¬è¿›è¡Œè°ƒæ•´\n",
    "# batched=True å…è®¸æˆ‘ä»¬ä¸€æ¬¡å¤„ç†å¤šä¸ªæ ·æœ¬ï¼Œè¿™å¯¹äºå¤§æ•°æ®é›†å°¤å…¶æœ‰æ•ˆã€‚\n",
    "train_dataset = train_dataset.map(adjust_labels, batched=True)\n",
    "test_dataset = test_dataset.map(adjust_labels, batched=True)\n",
    "\n",
    "# æ£€æŸ¥æ ‡ç­¾æ˜¯å¦å·²è°ƒæ•´åˆ°æ­£ç¡®çš„èŒƒå›´\n",
    "print(np.unique(train_dataset['label']))  # è¾“å‡ºè°ƒæ•´åçš„æ ‡ç­¾èŒƒå›´\n",
    "\n",
    "\n",
    "\n",
    "# ========== 2. åˆå§‹åŒ– tokenizer å’Œæ¨¡å‹ ==========\n",
    "MODEL_NAME = \"bert-base-multilingual-cased\"\n",
    "tokenizer = BertTokenizer.from_pretrained(MODEL_NAME)\n",
    "model = BertForSequenceClassification.from_pretrained(MODEL_NAME, num_labels=5)\n",
    "\n",
    "# ========== 3. æ•°æ®å¤„ç† ==========\n",
    "def preprocess_function(examples):\n",
    "    return tokenizer(examples[\"text\"], padding=\"max_length\", truncation=True, max_length=128)\n",
    "\n",
    "train_dataset = train_dataset.map(preprocess_function, batched=True)\n",
    "test_dataset = test_dataset.map(preprocess_function, batched=True)\n",
    "\n",
    "# ========== 4. å®šä¹‰è¯„ä¼°å‡½æ•° ==========\n",
    "def compute_metrics(p):\n",
    "    predictions, labels = p\n",
    "    predictions = np.argmax(predictions, axis=1)\n",
    "    acc = accuracy_score(labels, predictions)\n",
    "    return {\"accuracy\": acc}\n",
    "\n",
    "# ========== 5. è®¾ç½®è®­ç»ƒå‚æ•° ==========\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=64,\n",
    "    num_train_epochs=3,\n",
    "    weight_decay=0.01,\n",
    "    logging_dir=\"./logs\",\n",
    "    logging_steps=10,\n",
    "    save_steps=500,\n",
    "    save_total_limit=2,\n",
    ")\n",
    "\n",
    "# ========== 6. å®šä¹‰ Trainer ==========\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=test_dataset,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "# ========== 7. è®­ç»ƒæ¨¡å‹ ==========\n",
    "train_results = trainer.train()\n",
    "\n",
    "# ========== 8. å¯è§†åŒ–è®­ç»ƒè¿‡ç¨‹ ==========\n",
    "# æå–è®­ç»ƒè¿‡ç¨‹ä¸­çš„å‡†ç¡®ç‡ã€æŸå¤±å’Œæƒ…æ„Ÿå¼ºåº¦\n",
    "train_loss = train_results.metrics[\"train_loss\"]\n",
    "eval_loss = train_results.metrics[\"eval_loss\"]\n",
    "train_accuracy = train_results.metrics[\"eval_accuracy\"]\n",
    "\n",
    "# ç»˜åˆ¶æŸå¤±ï¼ˆLossï¼‰æ›²çº¿\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(train_results.history['eval_loss'], label='Validation Loss')\n",
    "plt.plot(train_results.history['train_loss'], label='Training Loss')\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.legend()\n",
    "plt.title(\"Loss Curve\")\n",
    "\n",
    "# ç»˜åˆ¶å‡†ç¡®ç‡æ›²çº¿\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(train_results.history['eval_accuracy'], label='Validation Accuracy')\n",
    "plt.plot(train_results.history['train_accuracy'], label='Training Accuracy')\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.legend()\n",
    "plt.title(\"Accuracy Curve\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# ========== 9. ä¿å­˜æ¨¡å‹ ==========\n",
    "model.save_pretrained(\"./sentiment_model\")\n",
    "tokenizer.save_pretrained(\"./sentiment_model\")\n",
    "\n",
    "# ========== 10. æ¨ç†ä¸æƒ…æ„Ÿå¼ºåº¦è®¡ç®— ==========\n",
    "# æµ‹è¯•æ¨¡å‹é¢„æµ‹\n",
    "inputs = tokenizer(\"æˆ‘ä»Šå¤©å¿ƒæƒ…éå¸¸å¥½\", return_tensors=\"pt\")\n",
    "outputs = model(**inputs)\n",
    "logits = outputs.logits\n",
    "predicted_class = torch.argmax(logits, dim=-1)\n",
    "predicted_label = predicted_class.item()\n",
    "\n",
    "# è¾“å‡ºé¢„æµ‹æƒ…æ„Ÿæ ‡ç­¾å’Œå¼ºåº¦ï¼ˆå¯ä»¥é€šè¿‡softmaxè·å¾—æ¦‚ç‡ä½œä¸ºæƒ…æ„Ÿå¼ºåº¦ï¼‰\n",
    "probabilities = torch.nn.functional.softmax(logits, dim=-1)\n",
    "predicted_probabilities = probabilities.detach().cpu().numpy()\n",
    "\n",
    "print(f\"Predicted Emotion: {predicted_label} (0: Very Negative, 1: Negative, 2: Neutral, 3: Positive, 4: Very Positive)\")\n",
    "print(f\"Emotion Intensity: {predicted_probabilities}\")\n",
    "\n",
    "# ç»˜åˆ¶æƒ…æ„Ÿå¼ºåº¦å›¾\n",
    "plt.bar(np.arange(5), predicted_probabilities[0])\n",
    "plt.xticks(np.arange(5), ['Very Negative', 'Negative', 'Neutral', 'Positive', 'Very Positive'])\n",
    "plt.xlabel(\"Emotion Class\")\n",
    "plt.ylabel(\"Intensity\")\n",
    "plt.title(\"Emotion Intensity\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[4, 3, 3, 4, 3, 2, 4, 4, 4, 2]\n",
      "[0 1 2 3 4]\n"
     ]
    }
   ],
   "source": [
    "print(train_dataset['label'][:10])  # Check first 10 labels\n",
    "print(np.unique(train_dataset['label']))  # Check unique label values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "è®­ç»ƒé›†æ ‡ç­¾åˆ†å¸ƒ: Counter({np.int64(2): 16826, np.int64(3): 8940, np.int64(4): 8820, np.int64(1): 4791, np.int64(0): 4033})\n",
      "éªŒè¯é›†æ ‡ç­¾åˆ†å¸ƒ: Counter({np.int64(2): 2111, np.int64(3): 1158, np.int64(4): 1117, np.int64(1): 551, np.int64(0): 489})\n"
     ]
    }
   ],
   "source": [
    "#æ£€æŸ¥æ ‡ç­¾åˆ†å¸ƒï¼š\n",
    "from collections import Counter\n",
    "print(\"è®­ç»ƒé›†æ ‡ç­¾åˆ†å¸ƒ:\", Counter(train_data['labels'].numpy()))\n",
    "print(\"éªŒè¯é›†æ ‡ç­¾åˆ†å¸ƒ:\", Counter(eval_data['labels'].numpy()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('./go_emotion_model_2\\\\tokenizer_config.json',\n",
       " './go_emotion_model_2\\\\special_tokens_map.json',\n",
       " './go_emotion_model_2\\\\vocab.txt',\n",
       " './go_emotion_model_2\\\\added_tokens.json')"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.save_pretrained('./go_emotions_model_2')\n",
    "tokenizer.save_pretrained('./go_emotion_model_2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#å­¦ä¹ ç‡è¿‡é«˜å¯èƒ½å¯¼è‡´æ¨¡å‹æ— æ³•æ”¶æ•›ï¼Œè¿‡ä½å¯èƒ½å¯¼è‡´è®­ç»ƒè¿‡æ…¢ã€‚"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
