{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 进行微调和优化"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. 准备数据集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e:\\demo\\EmotionApp\\backend\\EmotionAppBackend\\Algorithm\\HuggingFace_1\\hugging_face\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from transformers import Trainer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['sentence', 'label', 'idx'],\n",
      "        num_rows: 67349\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['sentence', 'label', 'idx'],\n",
      "        num_rows: 872\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['sentence', 'label', 'idx'],\n",
      "        num_rows: 1821\n",
      "    })\n",
      "})\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map:   0%|          | 0/67349 [00:00<?, ? examples/s]\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Class label 5 greater than configured num_classes 2",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 29\u001b[39m\n\u001b[32m     26\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m tokenizer(examples[\u001b[33m'\u001b[39m\u001b[33msentence\u001b[39m\u001b[33m'\u001b[39m], padding=\u001b[33m'\u001b[39m\u001b[33mmax_length\u001b[39m\u001b[33m'\u001b[39m, truncation=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m     28\u001b[39m \u001b[38;5;66;03m# 对训练数据进行预处理\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m29\u001b[39m train_dataset = \u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mtrain\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmap\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpreprocess_function\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatched\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[32m     30\u001b[39m train_dataset.set_format(\u001b[38;5;28mtype\u001b[39m=\u001b[33m'\u001b[39m\u001b[33mtorch\u001b[39m\u001b[33m'\u001b[39m, columns=[\u001b[33m'\u001b[39m\u001b[33minput_ids\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mattention_mask\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mlabel\u001b[39m\u001b[33m'\u001b[39m])\n\u001b[32m     32\u001b[39m \u001b[38;5;66;03m# 对验证数据进行预处理\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32me:\\demo\\EmotionApp\\backend\\EmotionAppBackend\\Algorithm\\HuggingFace_1\\hugging_face\\Lib\\site-packages\\datasets\\arrow_dataset.py:557\u001b[39m, in \u001b[36mtransmit_format.<locals>.wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    550\u001b[39m self_format = {\n\u001b[32m    551\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mtype\u001b[39m\u001b[33m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m._format_type,\n\u001b[32m    552\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mformat_kwargs\u001b[39m\u001b[33m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m._format_kwargs,\n\u001b[32m    553\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mcolumns\u001b[39m\u001b[33m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m._format_columns,\n\u001b[32m    554\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33moutput_all_columns\u001b[39m\u001b[33m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m._output_all_columns,\n\u001b[32m    555\u001b[39m }\n\u001b[32m    556\u001b[39m \u001b[38;5;66;03m# apply actual function\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m557\u001b[39m out: Union[\u001b[33m\"\u001b[39m\u001b[33mDataset\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mDatasetDict\u001b[39m\u001b[33m\"\u001b[39m] = \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    558\u001b[39m datasets: \u001b[38;5;28mlist\u001b[39m[\u001b[33m\"\u001b[39m\u001b[33mDataset\u001b[39m\u001b[33m\"\u001b[39m] = \u001b[38;5;28mlist\u001b[39m(out.values()) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(out, \u001b[38;5;28mdict\u001b[39m) \u001b[38;5;28;01melse\u001b[39;00m [out]\n\u001b[32m    559\u001b[39m \u001b[38;5;66;03m# re-apply format to the output\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32me:\\demo\\EmotionApp\\backend\\EmotionAppBackend\\Algorithm\\HuggingFace_1\\hugging_face\\Lib\\site-packages\\datasets\\arrow_dataset.py:3074\u001b[39m, in \u001b[36mDataset.map\u001b[39m\u001b[34m(self, function, with_indices, with_rank, input_columns, batched, batch_size, drop_last_batch, remove_columns, keep_in_memory, load_from_cache_file, cache_file_name, writer_batch_size, features, disable_nullable, fn_kwargs, num_proc, suffix_template, new_fingerprint, desc)\u001b[39m\n\u001b[32m   3068\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m transformed_dataset \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m   3069\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m hf_tqdm(\n\u001b[32m   3070\u001b[39m         unit=\u001b[33m\"\u001b[39m\u001b[33m examples\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m   3071\u001b[39m         total=pbar_total,\n\u001b[32m   3072\u001b[39m         desc=desc \u001b[38;5;129;01mor\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mMap\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m   3073\u001b[39m     ) \u001b[38;5;28;01mas\u001b[39;00m pbar:\n\u001b[32m-> \u001b[39m\u001b[32m3074\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mrank\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdone\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcontent\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mDataset\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_map_single\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mdataset_kwargs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m   3075\u001b[39m \u001b[43m            \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mdone\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m   3076\u001b[39m \u001b[43m                \u001b[49m\u001b[43mshards_done\u001b[49m\u001b[43m \u001b[49m\u001b[43m+\u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m1\u001b[39;49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32me:\\demo\\EmotionApp\\backend\\EmotionAppBackend\\Algorithm\\HuggingFace_1\\hugging_face\\Lib\\site-packages\\datasets\\arrow_dataset.py:3531\u001b[39m, in \u001b[36mDataset._map_single\u001b[39m\u001b[34m(shard, function, with_indices, with_rank, input_columns, batched, batch_size, drop_last_batch, remove_columns, keep_in_memory, cache_file_name, writer_batch_size, features, disable_nullable, fn_kwargs, new_fingerprint, rank, offset)\u001b[39m\n\u001b[32m   3529\u001b[39m         writer.write_table(batch.to_arrow())\n\u001b[32m   3530\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m3531\u001b[39m         \u001b[43mwriter\u001b[49m\u001b[43m.\u001b[49m\u001b[43mwrite_batch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   3532\u001b[39m num_examples_progress_update += num_examples_in_batch\n\u001b[32m   3533\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m time.time() > _time + config.PBAR_REFRESH_TIME_INTERVAL:\n",
      "\u001b[36mFile \u001b[39m\u001b[32me:\\demo\\EmotionApp\\backend\\EmotionAppBackend\\Algorithm\\HuggingFace_1\\hugging_face\\Lib\\site-packages\\datasets\\arrow_writer.py:606\u001b[39m, in \u001b[36mArrowWriter.write_batch\u001b[39m\u001b[34m(self, batch_examples, writer_batch_size)\u001b[39m\n\u001b[32m    604\u001b[39m         col_try_type = try_features[col] \u001b[38;5;28;01mif\u001b[39;00m try_features \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m col \u001b[38;5;129;01min\u001b[39;00m try_features \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    605\u001b[39m         typed_sequence = OptimizedTypedSequence(col_values, \u001b[38;5;28mtype\u001b[39m=col_type, try_type=col_try_type, col=col)\n\u001b[32m--> \u001b[39m\u001b[32m606\u001b[39m         arrays.append(\u001b[43mpa\u001b[49m\u001b[43m.\u001b[49m\u001b[43marray\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtyped_sequence\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[32m    607\u001b[39m         inferred_features[col] = typed_sequence.get_inferred_type()\n\u001b[32m    608\u001b[39m schema = inferred_features.arrow_schema \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.pa_writer \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m.schema\n",
      "\u001b[36mFile \u001b[39m\u001b[32me:\\demo\\EmotionApp\\backend\\EmotionAppBackend\\Algorithm\\HuggingFace_1\\hugging_face\\Lib\\site-packages\\pyarrow\\array.pxi:252\u001b[39m, in \u001b[36mpyarrow.lib.array\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32me:\\demo\\EmotionApp\\backend\\EmotionAppBackend\\Algorithm\\HuggingFace_1\\hugging_face\\Lib\\site-packages\\pyarrow\\array.pxi:114\u001b[39m, in \u001b[36mpyarrow.lib._handle_arrow_array_protocol\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32me:\\demo\\EmotionApp\\backend\\EmotionAppBackend\\Algorithm\\HuggingFace_1\\hugging_face\\Lib\\site-packages\\datasets\\arrow_writer.py:244\u001b[39m, in \u001b[36mTypedSequence.__arrow_array__\u001b[39m\u001b[34m(self, type)\u001b[39m\n\u001b[32m    239\u001b[39m     \u001b[38;5;66;03m# otherwise we can finally use the user's type\u001b[39;00m\n\u001b[32m    240\u001b[39m     \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mtype\u001b[39m \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    241\u001b[39m         \u001b[38;5;66;03m# We use cast_array_to_feature to support casting to custom types like Audio and Image\u001b[39;00m\n\u001b[32m    242\u001b[39m         \u001b[38;5;66;03m# Also, when trying type \"string\", we don't want to convert integers or floats to \"string\".\u001b[39;00m\n\u001b[32m    243\u001b[39m         \u001b[38;5;66;03m# We only do it if trying_type is False - since this is what the user asks for.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m244\u001b[39m         out = \u001b[43mcast_array_to_feature\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    245\u001b[39m \u001b[43m            \u001b[49m\u001b[43mout\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mtype\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mallow_primitive_to_str\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtrying_type\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mallow_decimal_to_str\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtrying_type\u001b[49m\n\u001b[32m    246\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    247\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m out\n\u001b[32m    248\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m (\n\u001b[32m    249\u001b[39m     \u001b[38;5;167;01mTypeError\u001b[39;00m,\n\u001b[32m    250\u001b[39m     pa.lib.ArrowInvalid,\n\u001b[32m    251\u001b[39m     pa.lib.ArrowNotImplementedError,\n\u001b[32m    252\u001b[39m ) \u001b[38;5;28;01mas\u001b[39;00m e:  \u001b[38;5;66;03m# handle type errors and overflows\u001b[39;00m\n\u001b[32m    253\u001b[39m     \u001b[38;5;66;03m# Ignore ArrowNotImplementedError caused by trying type, otherwise re-raise\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32me:\\demo\\EmotionApp\\backend\\EmotionAppBackend\\Algorithm\\HuggingFace_1\\hugging_face\\Lib\\site-packages\\datasets\\table.py:1798\u001b[39m, in \u001b[36m_wrap_for_chunked_arrays.<locals>.wrapper\u001b[39m\u001b[34m(array, *args, **kwargs)\u001b[39m\n\u001b[32m   1796\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m pa.chunked_array([func(chunk, *args, **kwargs) \u001b[38;5;28;01mfor\u001b[39;00m chunk \u001b[38;5;129;01min\u001b[39;00m array.chunks])\n\u001b[32m   1797\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1798\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43marray\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32me:\\demo\\EmotionApp\\backend\\EmotionAppBackend\\Algorithm\\HuggingFace_1\\hugging_face\\Lib\\site-packages\\datasets\\table.py:1996\u001b[39m, in \u001b[36mcast_array_to_feature\u001b[39m\u001b[34m(array, feature, allow_primitive_to_str, allow_decimal_to_str)\u001b[39m\n\u001b[32m   1994\u001b[39m     array = array.storage\n\u001b[32m   1995\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(feature, \u001b[33m\"\u001b[39m\u001b[33mcast_storage\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m-> \u001b[39m\u001b[32m1996\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfeature\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcast_storage\u001b[49m\u001b[43m(\u001b[49m\u001b[43marray\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1998\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m pa.types.is_struct(array.type):\n\u001b[32m   1999\u001b[39m     \u001b[38;5;66;03m# feature must be a dict or Sequence(subfeatures_dict)\u001b[39;00m\n\u001b[32m   2000\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(feature, Sequence) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(feature.feature, \u001b[38;5;28mdict\u001b[39m):\n",
      "\u001b[36mFile \u001b[39m\u001b[32me:\\demo\\EmotionApp\\backend\\EmotionAppBackend\\Algorithm\\HuggingFace_1\\hugging_face\\Lib\\site-packages\\datasets\\features\\features.py:1131\u001b[39m, in \u001b[36mClassLabel.cast_storage\u001b[39m\u001b[34m(self, storage)\u001b[39m\n\u001b[32m   1129\u001b[39m     min_max = pc.min_max(storage).as_py()\n\u001b[32m   1130\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m min_max[\u001b[33m\"\u001b[39m\u001b[33mmax\u001b[39m\u001b[33m\"\u001b[39m] \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m min_max[\u001b[33m\"\u001b[39m\u001b[33mmax\u001b[39m\u001b[33m\"\u001b[39m] >= \u001b[38;5;28mself\u001b[39m.num_classes:\n\u001b[32m-> \u001b[39m\u001b[32m1131\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m   1132\u001b[39m             \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mClass label \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmin_max[\u001b[33m'\u001b[39m\u001b[33mmax\u001b[39m\u001b[33m'\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m greater than configured num_classes \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m.num_classes\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m   1133\u001b[39m         )\n\u001b[32m   1134\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(storage, pa.StringArray):\n\u001b[32m   1135\u001b[39m     storage = pa.array(\n\u001b[32m   1136\u001b[39m         [\u001b[38;5;28mself\u001b[39m._strval2int(label) \u001b[38;5;28;01mif\u001b[39;00m label \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01mfor\u001b[39;00m label \u001b[38;5;129;01min\u001b[39;00m storage.to_pylist()]\n\u001b[32m   1137\u001b[39m     )\n",
      "\u001b[31mValueError\u001b[39m: Class label 5 greater than configured num_classes 2"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "from transformers import BertTokenizer, BertForSequenceClassification, TrainingArguments, Trainer\n",
    "import torch\n",
    "\n",
    "# 加载 Stanford Sentiment Treebank 数据集（sst2 为二分类数据集）\n",
    "dataset = load_dataset(\"glue\", \"sst2\")\n",
    "\n",
    "# 打印数据集的结构和内容\n",
    "print(dataset)\n",
    "\n",
    "# 加载预训练模型和分词器\n",
    "model_name = \"nlptown/bert-base-multilingual-uncased-sentiment\"\n",
    "tokenizer = BertTokenizer.from_pretrained(model_name)\n",
    "\n",
    "# 加载 BERT 模型\n",
    "model = BertForSequenceClassification.from_pretrained(model_name)\n",
    "\n",
    "# 将分类层调整为 2 个类别（POSITIVE, NEGATIVE）\n",
    "model.classifier = torch.nn.Linear(model.config.hidden_size, 2)  # sst2 只有 2 个类别：POSITIVE 和 NEGATIVE\n",
    "\n",
    "# 数据处理函数\n",
    "def preprocess_function(examples):\n",
    "    # 映射 SST2 标签为 1 和 5 星\n",
    "    label_mapping = {0: 1, 1: 5}  # 假设 0 = very negative, 1 = very positive\n",
    "    examples[\"label\"] = [label_mapping[label] for label in examples[\"label\"]]\n",
    "    return tokenizer(examples['sentence'], padding='max_length', truncation=True)\n",
    "\n",
    "# 对训练数据进行预处理\n",
    "train_dataset = dataset['train'].map(preprocess_function, batched=True)\n",
    "train_dataset.set_format(type='torch', columns=['input_ids', 'attention_mask', 'label'])\n",
    "\n",
    "# 对验证数据进行预处理\n",
    "eval_dataset = dataset['validation'].map(preprocess_function, batched=True)\n",
    "eval_dataset.set_format(type='torch', columns=['input_ids', 'attention_mask', 'label'])\n",
    "\n",
    "# 打印预处理后的数据集\n",
    "print(train_dataset[0])  # 打印第一个样本\n",
    "\n",
    "# 设置训练参数\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='./results',          # 输出目录\n",
    "    evaluation_strategy=\"epoch\",     # 每个epoch后评估\n",
    "    per_device_train_batch_size=8,   # 训练批次大小\n",
    "    per_device_eval_batch_size=8,    # 验证批次大小\n",
    "    num_train_epochs=3,              # 训练轮数\n",
    "    weight_decay=0.01,               # 权重衰减\n",
    "    logging_dir='./logs',            # 日志目录\n",
    "    logging_steps=10,                # 每10步记录日志\n",
    ")\n",
    "\n",
    "# 创建 Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,                         # 模型\n",
    "    args=training_args,                  # 训练参数\n",
    "    train_dataset=train_dataset,         # 训练数据\n",
    "    eval_dataset=eval_dataset,           # 验证数据\n",
    ")\n",
    "\n",
    "# 开始训练\n",
    "trainer.train()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'load_dataset' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m\n",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[19]\u001b[39m\u001b[32m, line 8\u001b[39m\n",
      "\u001b[32m      1\u001b[39m train_data = [\n",
      "\u001b[32m      2\u001b[39m     {\u001b[33m\"\u001b[39m\u001b[33mtext\u001b[39m\u001b[33m\"\u001b[39m: \u001b[33m\"\u001b[39m\u001b[33m今天心情很好，感觉很开心\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mlabel\u001b[39m\u001b[33m\"\u001b[39m: \u001b[32m4\u001b[39m},  \u001b[38;5;66;03m# 4 stars -> POSITIVE\u001b[39;00m\n",
      "\u001b[32m      3\u001b[39m     {\u001b[33m\"\u001b[39m\u001b[33mtext\u001b[39m\u001b[33m\"\u001b[39m: \u001b[33m\"\u001b[39m\u001b[33m今天很沮丧，压力很大\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mlabel\u001b[39m\u001b[33m\"\u001b[39m: \u001b[32m2\u001b[39m},  \u001b[38;5;66;03m# 2 stars -> NEGATIVE\u001b[39;00m\n",
      "\u001b[32m   (...)\u001b[39m\u001b[32m      6\u001b[39m     {\u001b[33m\"\u001b[39m\u001b[33mtext\u001b[39m\u001b[33m\"\u001b[39m: \u001b[33m\"\u001b[39m\u001b[33m今天失业了，心情很糟糕\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mlabel\u001b[39m\u001b[33m\"\u001b[39m: \u001b[32m1\u001b[39m}  \u001b[38;5;66;03m# 1 star -> NEGATIVE\u001b[39;00m\n",
      "\u001b[32m      7\u001b[39m ]\n",
      "\u001b[32m----> \u001b[39m\u001b[32m8\u001b[39m train_data = \u001b[43mload_dataset\u001b[49m(\u001b[33m\"\u001b[39m\u001b[33mamazon_polarity\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\n",
      "\u001b[31mNameError\u001b[39m: name 'load_dataset' is not defined"
     ]
    }
   ],
   "source": [
    "# train_data = [\n",
    "#     {\"text\": \"今天心情很好，感觉很开心\", \"label\": 4},  # 4 stars -> POSITIVE\n",
    "#     {\"text\": \"今天很沮丧，压力很大\", \"label\": 2},  # 2 stars -> NEGATIVE\n",
    "#     {\"text\": \"天气不错，但心情平静\", \"label\": 3},  # 3 stars -> NEUTRAL\n",
    "#     {\"text\": \"和朋友一起很开心，今天真是太好了\", \"label\": 5},  # 5 stars -> POSITIVE\n",
    "#     {\"text\": \"今天失业了，心情很糟糕\", \"label\": 1}  # 1 star -> NEGATIVE\n",
    "# ]\n",
    "train_data = load_dataset(\"amazon_polarity\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. 加载预训练模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer, BertForSequenceClassification, Trainer, TrainingArguments\n",
    "from datasets import load_dataset, Dataset\n",
    "\n",
    "# 加载预训练模型和分词器\n",
    "model_name = \"nlptown/bert-base-multilingual-uncased-sentiment\"\n",
    "tokenizer = BertTokenizer.from_pretrained(model_name)\n",
    "model = BertForSequenceClassification.from_pretrained(model_name, num_labels=3)  # 假设标签有 3 个：POSITIVE, NEGATIVE, NEUTRAL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. 数据预处理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 处理数据集\n",
    "def preprocess_function(examples):\n",
    "    return tokenizer(examples['text'], padding='max_length', truncation=True)\n",
    "\n",
    "# 将训练数据转化为 Hugging Face 数据集格式\n",
    "train_dataset = Dataset.from_pandas(pd.DataFrame(train_data))\n",
    "train_dataset = train_dataset.map(preprocess_function, batched=True)\n",
    "train_dataset.set_format(type='torch', columns=['input_ids', 'attention_mask', 'label'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. 训练设置"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 设置训练参数\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",          # 输出目录\n",
    "    evaluation_strategy=\"epoch\",     # 每个 epoch 后进行评估\n",
    "    learning_rate=2e-5,              # 学习率\n",
    "    per_device_train_batch_size=8,   # 每个设备的训练批大小\n",
    "    num_train_epochs=3,              # 训练 epoch 数量\n",
    "    weight_decay=0.01,               # 权重衰减\n",
    "    logging_dir='./logs',            # 日志目录\n",
    ")\n",
    "\n",
    "# 使用 Trainer 进行训练\n",
    "trainer = Trainer(\n",
    "    model=model,                     # 微调的模型\n",
    "    args=training_args,              # 训练参数\n",
    "    train_dataset=train_dataset,     # 训练数据集\n",
    "    eval_dataset=train_dataset,      # 评估数据集（可选，通常你会有单独的验证集）\n",
    ")\n",
    "\n",
    "# 开始训练\n",
    "trainer.train()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. 评估和保存模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 评估模型\n",
    "trainer.evaluate()\n",
    "\n",
    "# 保存模型\n",
    "model.save_pretrained(\"./fine_tuned_model\")\n",
    "tokenizer.save_pretrained(\"./fine_tuned_model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. 使用微调后的模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 加载微调后的模型\n",
    "fine_tuned_model = BertForSequenceClassification.from_pretrained(\"./fine_tuned_model\")\n",
    "fine_tuned_tokenizer = BertTokenizer.from_pretrained(\"./fine_tuned_model\")\n",
    "\n",
    "# 使用微调后的模型进行预测\n",
    "def predict(text):\n",
    "    inputs = fine_tuned_tokenizer(text, return_tensors=\"pt\", padding=True, truncation=True)\n",
    "    outputs = fine_tuned_model(**inputs)\n",
    "    prediction = outputs.logits.argmax(dim=-1).item()\n",
    "    return prediction\n",
    "\n",
    "# 测试预测\n",
    "sample_text = \"今天心情很好，感觉很开心\"\n",
    "print(f\"预测标签: {predict(sample_text)}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (hugging_face)",
   "language": "python",
   "name": "hugging_face"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
