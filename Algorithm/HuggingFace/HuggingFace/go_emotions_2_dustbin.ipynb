{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "❗不能直接用于继续训练（参数结构不同），但可以“迁移编码器部分”实现 “迁移式微调”（transfer learning）。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "from transformers import BertForSequenceClassification, BertTokenizer, Trainer, TrainingArguments\n",
    "from datasets import load_dataset\n",
    "import torch\n",
    "from torch import nn\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from transformers import EarlyStoppingCallback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. 标签映射\n",
    "five_class_mapping = {\n",
    "    'anger': 0, 'disgust': 0, 'fear': 0, 'sadness': 0,\n",
    "    'annoyance': 1, 'disappointment': 1,\n",
    "    'neutral': 2,\n",
    "    'approval': 3, 'joy': 3, 'love': 3, 'optimism': 3,\n",
    "    'admiration': 4, 'excitement': 4, 'gratitude': 4, 'pride': 4, 'relief': 4\n",
    "}\n",
    "id2label = {v: k for k, v in {\n",
    "    \"very negative\": 0,\n",
    "    \"negative\": 1,\n",
    "    \"neutral\": 2,\n",
    "    \"positive\": 3,\n",
    "    \"very positive\": 4\n",
    "}.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# 加载GoEmotions数据集\n",
    "dataset = load_dataset(\"go_emotions\")\n",
    "\n",
    "# 加载已保存的BERT模型和Tokenizer\n",
    "model_path = \"./go_emotions_model_1\"  # 已训练的GoEmotions模型路径\n",
    "tokenizer = BertTokenizer.from_pretrained(model_path)\n",
    "model = BertForSequenceClassification.from_pretrained(model_path, num_labels=5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 主任务：五分类情感标签；辅助任务：情绪强度（label数量相关性）\n",
    "def map_label(example):\n",
    "    label_ids = example['labels']\n",
    "    emotions = [dataset['train'].features['labels'].feature.names[i] for i in label_ids]\n",
    "    for emo in emotions:\n",
    "        if emo in five_class_mapping:\n",
    "            print(f\"Mapping for {emo}: {five_class_mapping[emo]}\")\n",
    "            return {\n",
    "                \"label\": five_class_mapping[emo],\n",
    "                \"intensity\": len(label_ids) / 5  # 简化情绪强度的构造\n",
    "            }\n",
    "    return {\"label\": 2, \"intensity\": 0.2}  # 默认neutral\n",
    "\n",
    "dataset = dataset.map(map_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "# print(\"训练集样本数:\", dataset['train']['label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer\n",
    "\n",
    "# 加载 tokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-chinese\")\n",
    "\n",
    "# 情感替换增强\n",
    "replacement_dict = {\n",
    "    \"bad\": \"terrible\", \"good\": \"excellent\", \"love\": \"adore\", \"hate\": \"despise\",\n",
    "    \"happy\": \"joyful\", \"sad\": \"depressed\", \"angry\": \"furious\"\n",
    "}\n",
    "\n",
    "def augment_text(example):\n",
    "    words = example['text'].split()\n",
    "    new_words = []\n",
    "    for w in words:\n",
    "        lower_w = w.lower()\n",
    "        if lower_w in replacement_dict:\n",
    "            replacement = replacement_dict[lower_w]\n",
    "            if w.istitle():\n",
    "                replacement = replacement.capitalize()\n",
    "            elif w.isupper():\n",
    "                replacement = replacement.upper()\n",
    "            new_words.append(replacement)\n",
    "        else:\n",
    "            new_words.append(w)\n",
    "    example['text'] = \" \".join(new_words)\n",
    "    return example\n",
    "\n",
    "# 情感替换增强\n",
    "dataset['train'] = dataset['train'].map(augment_text)\n",
    "\n",
    "# 编码函数\n",
    "def encode_examples(examples):\n",
    "    return tokenizer(\n",
    "        examples[\"text\"], \n",
    "        padding=True, \n",
    "        truncation=True, \n",
    "        return_tensors=\"pt\", \n",
    "        max_length=512\n",
    "    )\n",
    "\n",
    "# # 标签处理函数\n",
    "# def ensure_correct_label_format(example):\n",
    "#     if isinstance(example['label'], list):\n",
    "#         # 如果标签是列表，则取第一个元素\n",
    "#         example['label'] = example['label'][0]\n",
    "#     return example\n",
    "\n",
    "# # 确保标签格式正确\n",
    "# dataset['train'] = dataset['train'].map(ensure_correct_label_format)\n",
    "\n",
    "def check_labels(dataset):\n",
    "    print(dataset['train']['label'][:10])  # 检查前 10 个标签项\n",
    "\n",
    "# def check_data_structure(examples):\n",
    "#     print(f\"Input: {examples['text'][:5]}\")  # 仅打印前 5 个文本\n",
    "#     print(f\"Label: {examples['label']}\")  # 打印标签，直接作为一个整数\n",
    "#     return examples\n",
    "\n",
    "# # 调用修改后的函数\n",
    "# dataset['train'] = dataset['train'].map(check_data_structure)\n",
    "\n",
    "\n",
    "# Tokenize\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(\n",
    "        examples['text'], \n",
    "        padding=True, \n",
    "        truncation=True, \n",
    "        max_length=128\n",
    "    )\n",
    "\n",
    "# 批量 Tokenize\n",
    "dataset = dataset.map(tokenize_function, batched=True)\n",
    "\n",
    "# 设置数据格式\n",
    "# dataset.set_format(type=\"torch\", columns=[\"input_ids\", \"attention_mask\", \"label\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# 定义多任务模型（情绪分类 + 情绪强度预测）\n",
    "class MultiTaskModel(nn.Module):\n",
    "    def __init__(self, base_model):\n",
    "        super(MultiTaskModel, self).__init__()\n",
    "        self.bert = base_model.bert  # 使用已经训练过的BERT encoder\n",
    "        self.classifier = base_model.classifier  # 情绪分类头\n",
    "        # 情绪强度预测的任务头\n",
    "        self.intensity_head = nn.Linear(base_model.config.hidden_size, 1)  # 强度预测任务\n",
    "\n",
    "    def forward(self, input_ids, attention_mask, labels=None, intensity_labels=None):\n",
    "        outputs = self.bert(input_ids, attention_mask=attention_mask)\n",
    "        logits = self.classifier(outputs[1])  # 分类任务\n",
    "        intensity_logits = self.intensity_head(outputs[1])  # 强度预测任务\n",
    "\n",
    "        loss = None\n",
    "        if labels is not None and intensity_labels is not None:\n",
    "            # 计算分类任务损失\n",
    "            classification_loss = nn.CrossEntropyLoss()(logits, labels)\n",
    "            # 计算强度预测任务损失\n",
    "            intensity_loss = nn.MSELoss()(intensity_logits.squeeze(), intensity_labels)\n",
    "            loss = classification_loss + intensity_loss\n",
    "        elif labels is not None:\n",
    "            classification_loss = nn.CrossEntropyLoss()(logits, labels)\n",
    "            loss = classification_loss\n",
    "        elif intensity_labels is not None:\n",
    "            intensity_loss = nn.MSELoss()(intensity_logits.squeeze(), intensity_labels)\n",
    "            loss = intensity_loss\n",
    "\n",
    "        # 返回损失、分类预测和强度预测\n",
    "        return {\"loss\": loss, \"logits\": logits, \"intensity_logits\": intensity_logits}\n",
    "\n",
    "# 加载并修改为多任务模型\n",
    "multi_task_model = MultiTaskModel(model)\n",
    "\n",
    "# ========== 3. Tokenize ==========\n",
    "\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples['text'], padding=\"max_length\", truncation=True, max_length=512)\n",
    "\n",
    "dataset = dataset.map(tokenize_function, batched=True)\n",
    "\n",
    "# 设置格式供 PyTorch Trainer 使用，包括标签\n",
    "dataset.set_format(type=\"torch\", columns=[\"input_ids\", \"attention_mask\", \"label\", \"intensity\"])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from typing import Dict\n",
    "# import evaluate\n",
    "# from transformers.trainer_utils import EvalPrediction\n",
    "\n",
    "# # 加载评估指标\n",
    "# metric = evaluate.load(\"accuracy\")\n",
    "\n",
    "# def compute_metrics(pred:EvalPrediction) -> Dict[str, float]:\n",
    "#     # 预测输出\n",
    "#     # logits_cls, preds_intensity = pred.predictions  # 预测结果是一个 tuple\n",
    "#     # labels_cls = pred.label_ids[:, 0]  # 第0列是分类标签\n",
    "#     # labels_intensity = pred.label_ids[:, 1]  # 第1列是强度标签\n",
    "\n",
    "#     # preds_cls = np.argmax(logits_cls, axis=1)\n",
    "\n",
    "#     # # acc = accuracy_score(labels_cls, preds_cls)\n",
    "#     # # f1 = f1_score(labels_cls, preds_cls, average=\"weighted\")\n",
    "#     # # mse = mean_squared_error(labels_intensity, preds_intensity)\n",
    "#     # print(pred)\n",
    "#     # return {\n",
    "#     #     \"eval_accuracy\": metric.compute(predictions=preds_cls, references=labels_cls)[\"accuracy\"],\n",
    "#     #     \"eval_f1\": f1_score(labels_cls, preds_cls, average=\"weighted\"),\n",
    "#     #     \"eval_mse\": mean_squared_error(labels_intensity, preds_intensity),\n",
    "#     #     \"eval_loss\": pred.losses.mean(),\n",
    "#     #     # \"eval_accuracy\": acc,\n",
    "#     #     # \"accuracy\": acc,\n",
    "#     #     # \"f1\": f1,\n",
    "#     #     # \"mse\": mse,\n",
    "#     # }\n",
    "#     raise \"omg\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from transformers.trainer_utils import PredictionOutput\n",
    "# import numpy as np\n",
    "# from sklearn.metrics import accuracy_score, f1_score, mean_squared_error\n",
    "\n",
    "# def compute_metrics(pred):\n",
    "#     # 预测输出是一个包含两个部分的数组：分类和强度预测\n",
    "#     logits_cls = pred.predictions[:, 0]  # 分类预测结果\n",
    "#     preds_intensity = pred.predictions[:, 1]  # 强度预测结果\n",
    "    \n",
    "#     # 获取分类标签\n",
    "#     labels_cls = pred.label_ids[:, 0]  # 分类标签\n",
    "#     labels_intensity = pred.label_ids[:, 1]  # 强度标签\n",
    "\n",
    "#     # 如果 logits_cls 是一维数组，直接计算 argmax\n",
    "#     if logits_cls.ndim == 1:\n",
    "#         pred_cls = np.argmax(logits_cls)  # 不需要指定 axis\n",
    "#     else:\n",
    "#         pred_cls = np.argmax(logits_cls, axis=1)  # 分类预测\n",
    "\n",
    "#     # 计算分类准确率\n",
    "#     acc = accuracy_score(labels_cls, pred_cls)\n",
    "#     f1 = f1_score(labels_cls, pred_cls, average=\"weighted\")\n",
    "\n",
    "#     # 计算强度预测的均方误差\n",
    "#     mse = mean_squared_error(labels_intensity, preds_intensity)\n",
    "\n",
    "#     return {\n",
    "#         \"accuracy\": acc,\n",
    "#         \"f1\": f1,\n",
    "#         \"mse\": mse,  # 添加情绪强度的均方误差\n",
    "#     }\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from transformers.trainer_utils import PredictionOutput\n",
    "# # 测试 compute_metrics 是否正常\n",
    "# print(compute_metrics(PredictionOutput(\n",
    "#     predictions=np.array([[0.1, 0.9], [0.2, 0.8]]),  # 假设有两个预测值，一个是分类结果，一个是强度\n",
    "#     label_ids=np.array([[1, 5], [0, 3]]),  # 假设有两个标签，一个是分类标签，一个是强度标签\n",
    "#     metrics={}  # 空的 metrics 参数\n",
    "# )))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def compute_metrics(pred):\n",
    "#     labels = pred.label_ids\n",
    "#     preds = np.argmax(pred.predictions, axis=1)\n",
    "#     acc = accuracy_score(labels, preds)\n",
    "#     f1 = f1_score(labels, preds, average=\"weighted\")\n",
    "#     return {\"accuracy\": acc, \"f1\": f1}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset['validation'] = dataset['validation'].map(\n",
    "#     lambda x: tokenizer(x['text'], padding=True, truncation=True, max_length=512), batched=True\n",
    "# )\n",
    "# dataset['validation'].dic\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples['text'], padding=\"max_length\", truncation=True, max_length=512)\n",
    "\n",
    "dataset = dataset.map(tokenize_function, batched=True)\n",
    "\n",
    "# 设置格式供 PyTorch Trainer 使用\n",
    "dataset.set_format(type=\"torch\", columns=[\"input_ids\", \"attention_mask\", \"labels\"])\n",
    "\n",
    "# isinstance(dataset['validation'].map(\n",
    "#     lambda x: tokenizer(x['text'], padding=True, truncation=True, max_length=512), batched=True\n",
    "# ), dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "\n",
    "from typing import Dict\n",
    "import evaluate\n",
    "from transformers.trainer_utils import EvalPrediction\n",
    "\n",
    "# 加载评估指标\n",
    "metric = evaluate.load(\"accuracy\")\n",
    "\n",
    "def compute_metrics(pred:EvalPrediction) -> Dict[str, float]:\n",
    "    print(pred)\n",
    "    # 预测输出\n",
    "    logits_cls, preds_intensity = pred.predictions  # 预测结果是一个 tuple\n",
    "    labels_cls = pred.label_ids[:, 0]  # 第0列是分类标签\n",
    "    labels_intensity = pred.label_ids[:, 1]  # 第1列是强度标签\n",
    "    \n",
    "    preds_cls = np.argmax(logits_cls, axis=1)\n",
    "    \n",
    "    # 计算分类准确率\n",
    "    acc = accuracy_score(labels_cls, preds_cls)\n",
    "    f1 = f1_score(labels_cls, preds_cls, average=\"weighted\")\n",
    "    mse = mean_squared_error(labels_intensity, preds_intensity)\n",
    "    return {\n",
    "        \"eval_accuracy\": acc,\n",
    "        \"f1\": f1,\n",
    "        \"mse\": mse,  # 添加情绪强度的均方误差\n",
    "    }\n",
    "    \n",
    "\n",
    "# ========== 5. 训练参数 ==========\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='./results',          # 输出目录\n",
    "    num_train_epochs=3,              # 训练轮数\n",
    "    per_device_train_batch_size=1,#8,   # 每设备的批次大小\n",
    "    per_device_eval_batch_size=1,#16,   # 每设备的评估批次大小\n",
    "    logging_dir='./logs',            # 日志目录\n",
    "    logging_steps=10,\n",
    "    save_steps=100,\n",
    "    load_best_model_at_end=True,     # 在训练结束时加载最佳模型\n",
    "    eval_strategy= \"steps\",          # ✅ 新增：评估策略\n",
    "    eval_on_start=True,             # ✅ 新增：在训练开始时评估模型\n",
    "    eval_delay=10,\n",
    "    eval_steps=10,\n",
    "    metric_for_best_model=\"eval_accuracy\",\n",
    "    # eval_on_start=True,             # ✅ 新增：在训练开始时评估模型\n",
    "    # evaluation_strategy=\"epoch\",          # ✅ 新增：匹配 save_strategy\n",
    "    # batch_eval_metrics=[\"accuracy\", \"f1\"],  # ✅ 新增：评估指标\n",
    "    # batch_eval_metrics\n",
    "    greater_is_better=True,          # F1越高越好\n",
    "    save_strategy=\"steps\", \n",
    "    weight_decay=0.01,               # 权重衰减\n",
    "    warmup_steps=500,                # 预热步数\n",
    "    fp16=True,                       # 启用混合精度训练\n",
    "    max_steps=50,                  # 最大训练步数\n",
    ")\n",
    "\n",
    "# 使用Trainer类进行训练\n",
    "trainer = Trainer(\n",
    "    model=multi_task_model,                      # 预训练模型\n",
    "    args=training_args,                         # 训练参数\n",
    "    train_dataset=dataset['train'],             # 训练数据集\n",
    "    eval_dataset=dataset['validation'].select(range(100)),        # 验证数据集\n",
    "    tokenizer=tokenizer,                        # tokenizer\n",
    "    compute_metrics=compute_metrics,            # 自定义评估指标\n",
    "    callbacks=[EarlyStoppingCallback(early_stopping_patience=2)],  # 早停回调\n",
    ")\n",
    "\n",
    "# 训练模型\n",
    "trainer.train()\n",
    "\n",
    "# ========== 6. 训练结果可视化 ==========\n",
    "\n",
    "# 绘制训练过程中的损失曲线\n",
    "history = trainer.state.log_history\n",
    "train_losses = [entry['loss'] for entry in history if 'loss' in entry]\n",
    "eval_losses = [entry['eval_loss'] for entry in history if 'eval_loss' in entry]\n",
    "epochs = range(1, len(train_losses) + 1)\n",
    "\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(epochs, train_losses, label=\"Train Loss\", marker='o')\n",
    "plt.plot(epochs, eval_losses, label=\"Eval Loss\", marker='x')\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.title(\"Training and Evaluation Loss\")\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
